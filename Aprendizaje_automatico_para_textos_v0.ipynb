{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9be7036",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Film Junky Union, una nueva comunidad vanguardista para amantes de las películas clásicas, está desarrollando un sistema para filtrar y categorizar reseñas de películas.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "El objetivo es entrenar un modelo para detectar las críticas negativas de forma automática. Utilizarás un conjunto de datos de reseñas de películas de IMDB con leyendas de polaridad para construir un modelo para clasificar las reseñas positivas y negativas.\n",
    "\n",
    "## Condiciones\n",
    "\n",
    "Alcanzar un valor F1 de al menos 0.85.\n",
    "\n",
    "## Tabla de contenido\n",
    "\n",
    "1. Introducción\n",
    "2. Preprocesamiento\n",
    "3. Procesamiento de lenguaje natural\n",
    "4. Modelos de clasificación\n",
    "5. Conclusiones\n",
    "\n",
    "# Preprocesamiento\n",
    "\n",
    "Comenzaremos cargando los datos del archivo `imdb_reviews.tsv` y analizaremos tanto el contenido como las condiciones en las que se encuentran los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c24dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando librerias\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de6123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47331 entries, 0 to 47330\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   tconst           47331 non-null  object \n",
      " 1   title_type       47331 non-null  object \n",
      " 2   primary_title    47331 non-null  object \n",
      " 3   original_title   47331 non-null  object \n",
      " 4   start_year       47331 non-null  int64  \n",
      " 5   end_year         47331 non-null  object \n",
      " 6   runtime_minutes  47331 non-null  object \n",
      " 7   is_adult         47331 non-null  int64  \n",
      " 8   genres           47331 non-null  object \n",
      " 9   average_rating   47329 non-null  float64\n",
      " 10  votes            47329 non-null  float64\n",
      " 11  review           47331 non-null  object \n",
      " 12  rating           47331 non-null  int64  \n",
      " 13  sp               47331 non-null  object \n",
      " 14  pos              47331 non-null  int64  \n",
      " 15  ds_part          47331 non-null  object \n",
      " 16  idx              47331 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(10)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Leyendo archivo con extensión \"tsv\"\n",
    "df = pd.read_csv('/datasets/imdb_reviews.tsv',sep='\\t')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbb1421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>title_type</th>\n",
       "      <th>primary_title</th>\n",
       "      <th>original_title</th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_year</th>\n",
       "      <th>runtime_minutes</th>\n",
       "      <th>is_adult</th>\n",
       "      <th>genres</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>votes</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sp</th>\n",
       "      <th>pos</th>\n",
       "      <th>ds_part</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0068152</td>\n",
       "      <td>movie</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>1971</td>\n",
       "      <td>\\N</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Crime,Drama</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2218.0</td>\n",
       "      <td>The pakage implies that Warren Beatty and Gold...</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>8335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0068152</td>\n",
       "      <td>movie</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>1971</td>\n",
       "      <td>\\N</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Crime,Drama</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2218.0</td>\n",
       "      <td>How the hell did they get this made?! Presenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>8336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>\\N</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184.0</td>\n",
       "      <td>There is no real story the film seems more lik...</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>\\N</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184.0</td>\n",
       "      <td>Um .... a serious film about troubled teens in...</td>\n",
       "      <td>7</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>9280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>\\N</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184.0</td>\n",
       "      <td>I'm totally agree with GarryJohal from Singapo...</td>\n",
       "      <td>9</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>9281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>\\N</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184.0</td>\n",
       "      <td>This is the first movie I've seen from Singapo...</td>\n",
       "      <td>9</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>9282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>\\N</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184.0</td>\n",
       "      <td>Yes non-Singaporean's can't see what's the big...</td>\n",
       "      <td>9</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>9283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tt0035958</td>\n",
       "      <td>movie</td>\n",
       "      <td>'Gung Ho!': The Story of Carlson's Makin Islan...</td>\n",
       "      <td>'Gung Ho!': The Story of Carlson's Makin Islan...</td>\n",
       "      <td>1943</td>\n",
       "      <td>\\N</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>Drama,History,War</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>This true story of Carlson's Raiders is more o...</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>9903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tt0035958</td>\n",
       "      <td>movie</td>\n",
       "      <td>'Gung Ho!': The Story of Carlson's Makin Islan...</td>\n",
       "      <td>'Gung Ho!': The Story of Carlson's Makin Islan...</td>\n",
       "      <td>1943</td>\n",
       "      <td>\\N</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>Drama,History,War</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>Should have been titled 'Balderdash!' Little i...</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>9905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tt0035958</td>\n",
       "      <td>movie</td>\n",
       "      <td>'Gung Ho!': The Story of Carlson's Makin Islan...</td>\n",
       "      <td>'Gung Ho!': The Story of Carlson's Makin Islan...</td>\n",
       "      <td>1943</td>\n",
       "      <td>\\N</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>Drama,History,War</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>The movie 'Gung Ho!': The Story of Carlson's M...</td>\n",
       "      <td>4</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>9904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst title_type                                      primary_title  \\\n",
       "0  tt0068152      movie                                                  $   \n",
       "1  tt0068152      movie                                                  $   \n",
       "2  tt0313150      short                                               '15'   \n",
       "3  tt0313150      short                                               '15'   \n",
       "4  tt0313150      short                                               '15'   \n",
       "5  tt0313150      short                                               '15'   \n",
       "6  tt0313150      short                                               '15'   \n",
       "7  tt0035958      movie  'Gung Ho!': The Story of Carlson's Makin Islan...   \n",
       "8  tt0035958      movie  'Gung Ho!': The Story of Carlson's Makin Islan...   \n",
       "9  tt0035958      movie  'Gung Ho!': The Story of Carlson's Makin Islan...   \n",
       "\n",
       "                                      original_title  start_year end_year  \\\n",
       "0                                                  $        1971       \\N   \n",
       "1                                                  $        1971       \\N   \n",
       "2                                               '15'        2002       \\N   \n",
       "3                                               '15'        2002       \\N   \n",
       "4                                               '15'        2002       \\N   \n",
       "5                                               '15'        2002       \\N   \n",
       "6                                               '15'        2002       \\N   \n",
       "7  'Gung Ho!': The Story of Carlson's Makin Islan...        1943       \\N   \n",
       "8  'Gung Ho!': The Story of Carlson's Makin Islan...        1943       \\N   \n",
       "9  'Gung Ho!': The Story of Carlson's Makin Islan...        1943       \\N   \n",
       "\n",
       "  runtime_minutes  is_adult              genres  average_rating   votes  \\\n",
       "0             121         0  Comedy,Crime,Drama             6.3  2218.0   \n",
       "1             121         0  Comedy,Crime,Drama             6.3  2218.0   \n",
       "2              25         0  Comedy,Drama,Short             6.3   184.0   \n",
       "3              25         0  Comedy,Drama,Short             6.3   184.0   \n",
       "4              25         0  Comedy,Drama,Short             6.3   184.0   \n",
       "5              25         0  Comedy,Drama,Short             6.3   184.0   \n",
       "6              25         0  Comedy,Drama,Short             6.3   184.0   \n",
       "7              88         0   Drama,History,War             6.1  1240.0   \n",
       "8              88         0   Drama,History,War             6.1  1240.0   \n",
       "9              88         0   Drama,History,War             6.1  1240.0   \n",
       "\n",
       "                                              review  rating   sp  pos  \\\n",
       "0  The pakage implies that Warren Beatty and Gold...       1  neg    0   \n",
       "1  How the hell did they get this made?! Presenti...       1  neg    0   \n",
       "2  There is no real story the film seems more lik...       3  neg    0   \n",
       "3  Um .... a serious film about troubled teens in...       7  pos    1   \n",
       "4  I'm totally agree with GarryJohal from Singapo...       9  pos    1   \n",
       "5  This is the first movie I've seen from Singapo...       9  pos    1   \n",
       "6  Yes non-Singaporean's can't see what's the big...       9  pos    1   \n",
       "7  This true story of Carlson's Raiders is more o...       2  neg    0   \n",
       "8  Should have been titled 'Balderdash!' Little i...       2  neg    0   \n",
       "9  The movie 'Gung Ho!': The Story of Carlson's M...       4  neg    0   \n",
       "\n",
       "  ds_part   idx  \n",
       "0   train  8335  \n",
       "1   train  8336  \n",
       "2    test  2489  \n",
       "3    test  9280  \n",
       "4    test  9281  \n",
       "5    test  9282  \n",
       "6    test  9283  \n",
       "7   train  9903  \n",
       "8   train  9905  \n",
       "9   train  9904  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando contenido de los primeros 10 registros\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0e532",
   "metadata": {},
   "source": [
    "De acuerdo con el objetivo que solicita `Film Junky Union` no tenemos que hacer uso de toda la información presentada en `df`, por lo que las columnas de interes son las siguientes: \n",
    "\n",
    "- `review`: la cuál contiene las reseñas de las peliculas.\n",
    "- `pos`: reseña positiva (1) o negativa (0) la cuál es nuestra columna objetivo.\n",
    "- `ds_part`: Nos indica que registros serán utilizados para entrenamiento y cuales para testeo.\n",
    "\n",
    "Una vez teniendo dicha información podemos resaltar hasta este punto que no se tienen registros nulos en las columnas de interes, por lo que procederemos a reducir `df` únicamente con información de relevancia para el cumplimiento del objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68230c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47331 entries, 0 to 47330\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   review   47331 non-null  object\n",
      " 1   pos      47331 non-null  int64 \n",
      " 2   ds_part  47331 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Ajusta \"df\" con las columnas de interes\n",
    "df = df[['review','pos','ds_part']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6da16",
   "metadata": {},
   "source": [
    "Una vez que tenemos la información necesaria, verificaremos lo siguiente:\n",
    "\n",
    "- Validaremos que no hay duplicidad de registros.\n",
    "- Verificaremos el contenido de la columna `pos` y `ds_part` para validar que estas solo tengan dos categorías.\n",
    "\n",
    "Esto como parte del preprocesamiento a realizar antes de comenzar a trabajar con los textos de la columna `review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9655ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrando duplicidad de registros\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc02bbf",
   "metadata": {},
   "source": [
    "Como podemos observar, tenemos `90` registros duplicados los cuales pueden provocar un sesgo en el entrenamiento por lo que es importante excluirlos del `df` a procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a56c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminando registros ducplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b05cecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23680\n",
       "1    23561\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validando columna \"pos\"\n",
    "df['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb730de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    23758\n",
       "test     23483\n",
       "Name: ds_part, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validando columna \"ds_part\"\n",
    "df['ds_part'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858ca8f",
   "metadata": {},
   "source": [
    "Como podemos observar, tanto la columna `pos` como `ds_part` no cuentan con anomalías en sus datos. Hasta este punto podemos afirmar que tenemos los datos necesarios para poder comenzar a trabajar en el modelo de recomendación basado en las criticas de las películas.\n",
    "\n",
    "# PLN (Procesamiento de Lenguaje Natural)\n",
    "\n",
    "Para comenzar a trabajar con las reseñas o críticas, debemos recordar que tanto los modelos matemáticos como los ordenadores, únicamente pueden procesar números, por lo que es importante hacer una transformación de tipo de dato, sin embargo, este proceso va de la mano de ciertos pasos que permitiran hacer dicha conversión eficientemente y son:\n",
    "\n",
    "- `Tokenización`: Es el proceso mediante el cual las frases se dividen en tokes, es decir, palabras y símbolos separados.\n",
    "- `Lematización`: Es el proceso mediante el cual se basifican las palabras, es decir, las palabras conjugadas en los diferentes tiempos verbales se transformar en las palabras base.\n",
    "- `Stopwords`: Son aquellas palabras que no aportan información a las frases, es decir, palabras que no significan nada por si solas y son conocidas como \"palabras vacias\".\n",
    "\n",
    "Por cuestiones de procesamiento, haremos uso del proceso `TF-IDF` en el cual a través de la creación de una bolsa de palabras o corpus podemos calcular la frecuencia de aparición de las palabras en un texto y la frecuencia de aparición en el corpus del tal forma que podemos hacer dicha conversión de texto-números de una forma coherente y eficiente para los modelos de machine learning.\n",
    "\n",
    "Comenzaremos con el proceso de `tokenización` y `lematización` para lo cual tendremos que hacer uso de la librería `NLTK (Natural Language ToolKit)`, ademas también, requeriremos eliminar simbolos ya que no son de utilidad para el algoritmo.\n",
    "\n",
    "## Tokenización, lematización y eliminación de símbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9bb1bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "      <th>ds_part</th>\n",
       "      <th>lemmas_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pakage implies that Warren Beatty and Gold...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>the pakage implies that warren beatty and gold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How the hell did they get this made?! Presenti...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>how the hell did they get this made presenting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is no real story the film seems more lik...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>there is no real story the film seems more lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Um .... a serious film about troubled teens in...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>um a serious film about troubled teen in singa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm totally agree with GarryJohal from Singapo...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>i 'm totally agree with garryjohal from singap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is the first movie I've seen from Singapo...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>this is the first movie i 've seen from singap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes non-Singaporean's can't see what's the big...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>yes non singaporean 's ca n't see what 's the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This true story of Carlson's Raiders is more o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>this true story of carlson 's raider is more o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Should have been titled 'Balderdash!' Little i...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>should have been titled 'balderdash ' little i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The movie 'Gung Ho!': The Story of Carlson's M...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>the movie 'gung ho ' the story of carlson 's m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  pos ds_part  \\\n",
       "0  The pakage implies that Warren Beatty and Gold...    0   train   \n",
       "1  How the hell did they get this made?! Presenti...    0   train   \n",
       "2  There is no real story the film seems more lik...    0    test   \n",
       "3  Um .... a serious film about troubled teens in...    1    test   \n",
       "4  I'm totally agree with GarryJohal from Singapo...    1    test   \n",
       "5  This is the first movie I've seen from Singapo...    1    test   \n",
       "6  Yes non-Singaporean's can't see what's the big...    1    test   \n",
       "7  This true story of Carlson's Raiders is more o...    0   train   \n",
       "8  Should have been titled 'Balderdash!' Little i...    0   train   \n",
       "9  The movie 'Gung Ho!': The Story of Carlson's M...    0   train   \n",
       "\n",
       "                                       lemmas_review  \n",
       "0  the pakage implies that warren beatty and gold...  \n",
       "1  how the hell did they get this made presenting...  \n",
       "2  there is no real story the film seems more lik...  \n",
       "3  um a serious film about troubled teen in singa...  \n",
       "4  i 'm totally agree with garryjohal from singap...  \n",
       "5  this is the first movie i 've seen from singap...  \n",
       "6  yes non singaporean 's ca n't see what 's the ...  \n",
       "7  this true story of carlson 's raider is more o...  \n",
       "8  should have been titled 'balderdash ' little i...  \n",
       "9  the movie 'gung ho ' the story of carlson 's m...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceso de tokenización, lematización y eliminación de símbolos\n",
    "lemmatizer  = WordNetLemmatizer() \n",
    "pattern = r\"[^a-zA-Z']\"\n",
    "\n",
    "def tokens_lemmas(text):\n",
    "    text = re.sub(pattern,\" \",text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    result = ' '.join(lemmas)\n",
    "    return result\n",
    "\n",
    "df['lemmas_review'] = df['review'].apply(tokens_lemmas)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6b947",
   "metadata": {},
   "source": [
    "Ya que tenemos las reseñas de la forma correcta, comenzaremos con la creación de la bolsa de palabras usando `CountVectorizer` de `sklearn` lo cual nos permite crear un corpus de texto en una bolsa de palabras. En este punto al crear la bolsa de palabras debemos tener en consideración que esta debe contemplar únicamente los registros que en la columna `ds_part` tengan el dato `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364a9b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23758, 64299)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creando bolsa de palabras\n",
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "corpus = df[df['ds_part'] == 'train']['lemmas_review']\n",
    "tf_idf = count_tf_idf.fit(corpus)\n",
    "X_train = tf_idf.transform(corpus)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e4b59",
   "metadata": {},
   "source": [
    "Como podemos observar, la bolsa de palabras ya vectorizada consta de `23,758` registros los cuales coinciden por completo con la cantidad de registros categorizados por la columna `ds_part` como `train`.\n",
    "\n",
    "Ya que contamos con dicha bolsa de palabras, debemos considerar que para los registros de testeo, estos deben ser modificados bajo las mismas condiciones con las que fue construida la bolsa, es decir, deben ser transformadas a partir de `tf_idf` únicamente.\n",
    "\n",
    "A partir de este punto ya podemos comenzar con el entrenamiento de modelos de clasificación.\n",
    "\n",
    "# Modelos de clasificación\n",
    "\n",
    "Comenzaremos con los modelos de clasificación a partir de este punto empezando por la `regresión logística`.\n",
    "\n",
    "## Regresión logística\n",
    "\n",
    "Recordando los parámetros que recibe la `regresión logística`, para el parámetro `solver`, tenemos lo siguiente:\n",
    "\n",
    "- `liblinear`: Se aplica cuando se tiene un dataset pequeño ademas de una categorización binaria, es decir, uno contra el resto de resultados.\n",
    "- `newton-cg`, `sag`, `saga` and `lbfgs`: Se aplican cuando se tienen problemas de multiclase, es decir, que la clasificación va más allá de 2 clases de resultados.\n",
    "- `newton-cholesky`: Se usa cuando dentro del mismo dataset hay información categorica.\n",
    "\n",
    "Por lo que `liblinear` es el parámetro más adecuado por las características de los registros en `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340a4435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23758,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrando \"y_train\"\n",
    "y_train = df[df['ds_part'] == 'train']['pos']\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56130b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23483, 64299), (23483,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrando X_test, y_test\n",
    "corpus_test = df[df['ds_part'] == 'test']['lemmas_review']\n",
    "X_test = tf_idf.transform(corpus_test)\n",
    "y_test = df[df['ds_part'] == 'test']['pos']\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e0d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión obtenida en el algoritmo de \"Regresión logística es\": 0.8792402789066176\n"
     ]
    }
   ],
   "source": [
    "# Implementando modelo de clasificación\n",
    "lr_model = LogisticRegression(random_state=12345,solver='liblinear')\n",
    "lr_model.fit(X_train,y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_score = f1_score(y_test,y_pred_lr)\n",
    "print('La precisión obtenida en el algoritmo de \"Regresión logística\" es:',lr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92528a",
   "metadata": {},
   "source": [
    "Como podemos observar, obtenemos un resultado satisfactorio pues cumple con la condición establecida lo que nos quiere decir que la técnica de `procesamiento de lenguaje natural` es suficientemente buena para asegurar una exactitud de predicción aceptable. Aunque evaluaremos otros algoritmos de clasificación para que de igual forma obtengamos su desempeño.\n",
    "\n",
    "## Árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14afd574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'criterion': 'gini', 'max_depth': 9, 'min_samples_leaf': 16, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.7562507618291743\n",
      "Best estimator:  DecisionTreeClassifier(max_depth=9, min_samples_leaf=16, random_state=12345)\n"
     ]
    }
   ],
   "source": [
    "# Ajustando parámetros\n",
    "dtc_model = DecisionTreeClassifier(random_state=12345)\n",
    "params = {'criterion': ['gini','entropy'],\n",
    "          'splitter': ['best'],\n",
    "          'max_depth': range(1,10), \n",
    "          'min_samples_split': range(2,20,2), \n",
    "          'min_samples_leaf': range(2,20,2)}\n",
    "grid = GridSearchCV(dtc_model,params,cv=2,n_jobs=-1,scoring='f1')\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a169996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión obtenida en el algoritmo de \"Regresión logística\" es: 0.7565024479804162\n"
     ]
    }
   ],
   "source": [
    "# Implementando modelo de clasificación\n",
    "dtc_model = DecisionTreeClassifier(random_state=12345, criterion='gini', max_depth=9,\n",
    "                                   min_samples_leaf=16, min_samples_split=2,splitter='best')\n",
    "dtc_model.fit(X_train,y_train)\n",
    "y_pred_dtc = dtc_model.predict(X_test)\n",
    "dtc_score = f1_score(y_test,y_pred_dtc)\n",
    "print('La precisión obtenida en el algoritmo de \"Árboles de decisión\" es:',dtc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da3e00",
   "metadata": {},
   "source": [
    "## Bosques aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3494e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'criterion': 'gini', 'max_depth': 9, 'min_samples_leaf': 6, 'min_samples_split': 16, 'n_estimators': 151}\n",
      "Best score:  0.8309877083727699\n",
      "Best estimator:  RandomForestClassifier(max_depth=9, min_samples_leaf=6, min_samples_split=16,\n",
      "                       n_estimators=151, random_state=12345)\n"
     ]
    }
   ],
   "source": [
    "# Ajustando parámetros\n",
    "rfc_model = RandomForestClassifier(random_state=12345)\n",
    "params = {'criterion': ['gini','entropy'],\n",
    "          'max_depth': range(1,10),\n",
    "          'min_samples_split': range(2,20,2), \n",
    "          'min_samples_leaf': range(2,20,2)}\n",
    "grid = GridSearchCV(rfc_model,params,cv=2,n_jobs=-1,scoring='f1')\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67f2afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión obtenida en el algoritmo de \"Bosques aleatorios\" es: 0.8243831640058056\n"
     ]
    }
   ],
   "source": [
    "# Implementando modelo de clasificación\n",
    "rfc_model = RandomForestClassifier(random_state=12345,criterion='gini',max_depth=9,\n",
    "                                   min_samples_leaf=6,min_samples_split=16,n_jobs=-1)\n",
    "rfc_model.fit(X_train,y_train)\n",
    "y_pred_rfc = rfc_model.predict(X_test)\n",
    "rfc_score = f1_score(y_test,y_pred_rfc)\n",
    "print('La precisión obtenida en el algoritmo de \"Bosques aleatorios\" es:',rfc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49ca4a",
   "metadata": {},
   "source": [
    "Como podemos observar en tanto en `árboles de decisión` como en `bosques aleatorios`, se tiene un desempeño que se encuentra por debajo de la condicionante para considerarlo un modelo aceptablel, esto puede deberse a las limitantes de procesamiento por parte del ordenador ya que a mayor cantidad de parámetros se vuelve un proceso más y más complejo ademas de demandar consumo de procesamiento cada vez mayor.\n",
    "\n",
    "Sin embargo, aun tenemos un modelo más por probar que es un algoritmo que potencializa el gradiente.\n",
    "\n",
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2901b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.1, 'n_estimators': 200, 'num_leaves': 40}\n",
      "Best score:  0.8579249339910451\n",
      "Best estimator:  LGBMClassifier(n_estimators=200, num_leaves=40)\n"
     ]
    }
   ],
   "source": [
    "# Modelo de clasificación con potenciación de gradiente\n",
    "train_data = lgb.Dataset(X_train,label=y_train)\n",
    "test_data = lgb.Dataset(X_test,label=y_test)\n",
    "\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "\n",
    "params = {'learning_rate': [0.05, 0.1, 0.2],\n",
    "          'num_leaves': [20, 31, 40],\n",
    "          'n_estimators': [50, 100, 200]}\n",
    "\n",
    "grid = GridSearchCV(gbm_model,params,cv=2,n_jobs=-1,scoring='f1')\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7db5054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión obtenida en el algoritmo de \"LightGBM\" es: 0.8733208637986736\n"
     ]
    }
   ],
   "source": [
    "# Implementando modelo de clasificación\n",
    "gbm_model = lgb.LGBMClassifier(learning_rate=0.1, num_leaves=40, n_estimators=200)\n",
    "gbm_model.fit(X_train,y_train)\n",
    "y_pred_gbm = gbm_model.predict(X_test)\n",
    "gbm_score = f1_score(y_test,y_pred_gbm)\n",
    "print('La precisión obtenida en el algoritmo de \"LightGBM\" es:',gbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be306ebb",
   "metadata": {},
   "source": [
    "Como podemos observar, tanto `regresión logística` como `lightGBM` son los modelos que nos permiten una eficiencia de clasificación aceptable para la `Film Junky Union`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "717c10aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Desempeño</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Regresión logística</th>\n",
       "      <td>87.924028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>87.332086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bosques aleatorios</th>\n",
       "      <td>82.438316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Árboles de decisión</th>\n",
       "      <td>75.650245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Desempeño\n",
       "Regresión logística  87.924028\n",
       "LightGBM             87.332086\n",
       "Bosques aleatorios   82.438316\n",
       "Árboles de decisión  75.650245"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabla de desempeño\n",
    "pd.DataFrame(data={'Desempeño': [lr_score*100,dtc_score*100,rfc_score*100,gbm_score*100]},\n",
    "             index=['Regresión logística','Árboles de decisión','Bosques aleatorios','LightGBM']).sort_values(by=['Desempeño'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d99826",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Al trabajar este tipo de proyectos, es importante considerar la capacidad computacional del ordenador en el que uno se encuentre trabajando, esto es debido a que se trabaja con matrices lo cuál demanda capacidad de computo, por lo que se opto por trabajar con la creación de bolsa de palabras a partir de un corpus bajo el cálculo de `TF-IDF`, por otro lado tenemos los modelos de clasificación que pudimos observar que aquellos modelos basados en árboles, presentan una eficiencia menor contra la `regresión logística` y `LightGBM`, esto puede deberse a lo anteriormente menciona pues la capacidades de `árboles de decisión` y `bosques aleatorios` se ven limitadas por le host de procesamiento que en este caso es el ordenador personal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
